{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "f1ece82947e35fc60cffaae64fc6e659b49429d5"
   },
   "source": [
    "# CNN beats NN beats kNN, 99.5% vs. 98.5% vs. 97%\n",
    "Here's my second and third attempt classifying the MNIST handwritten digits. My first attempt used kNN and scored 97%. Next, I coded a fully connected feedforward neural network in C which scores 98.5%. Data augmentation pushes that to 99%. Third I added convolutions to my C program and score 99.5%!! \n",
    "\n",
    "UPDATE: Using TensorFlow (and GPUs) I built an ensemble of CNNs and scored an amazing 99.75% accuracy!! Check it out, [here][1].\n",
    "  \n",
    "UPDATE: I created a webpage where you can draw digits with your mouse and watch this network classify them, [here][2].\n",
    "\n",
    "## The Training Data\n",
    "Here are the first 54 training images. Kaggle's MNIST training dataset contains 42,000 images of size 28 x 28 grayscale pixels. Kaggle's MNIST test dataset contains 28,000 unlabeled images. Our model must learn to recognize digits from the training images and then predict the correct labels for the test images.\n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/digits54.png)\n",
    "\n",
    "[1]:https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist\n",
    "[2]:http://www.ccom.ucsd.edu/~cdeotte/programs/MNIST.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1de7a63d46c95834dffc7fca4133983e68bbc673"
   },
   "source": [
    "# NN Architecture - 98.5%\n",
    "The images have 784 features ( = 28 by 28 pixels each with grayscale color from 0.0 to 1.0). This input feeds into a hidden layer of 1000 neurons which feeds into another hidden layer of 1000 neurons. The output layer has 10 neurons. The architecture is 784-1000-1000-10.  \n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/MNISTnet2.jpeg)\n",
    "\n",
    "# Initialization\n",
    "For the hidden layer activations, I use ReLU. The output layer uses softmax regression with cross entropy cost. The network learns with stochastic back propagation gradient descent and the weights are initialized with Xavier-He initialization. Below is my C code: (also on GitHub [here][1])\n",
    "\n",
    "[1]:https://github.com/cdeotte/MNIST-CNN-99.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bd6832209bf3e059abe31ad1baaa11075b4557d"
   },
   "source": [
    "```\n",
    "int layerSizes[10] = {0,0,0,0,0,0,784,1000,1000,10};\n",
    "float* layers[10] = {0};\n",
    "float* errors[10] = {0};\n",
    "float*  weights[10] = {0};\n",
    "// INITIALIZATION\n",
    "void initNet(){\n",
    "    // ALOCATE MEMORY\n",
    "    layers[0] = (float*)malloc((layerSizes[0]+1) * sizeof(float));\n",
    "    errors[0] = (float*)malloc(layerSizes[0] * sizeof(float));\n",
    "    for (i=1;i<10;i++){\n",
    "        layers[i] = (float*)malloc((layerSizes[i]+1) * sizeof(float));\n",
    "        errors[i] = (float*)malloc(layerSizes[i] * sizeof(float));\n",
    "        weights[i] = (float*)malloc(layerSizes[i] * (layerSizes[i-1]+1) * sizeof(float));\n",
    "    }\n",
    "    // RANDOMIZE WEIGHTS AND BIAS\n",
    "    float scale;\n",
    "    for (i=0;i<10;i++) layers[i][layerSizes[i]]=1.0;\n",
    "    for (j=1;j<10;j++){\n",
    "        // XAVIER-HE INITIALIZATION\n",
    "        scale = 2.0 * sqrt(6.0/(layerSizes[j] + layerSizes[j-1]));\n",
    "        if (j!=9 && activation==1) scale = scale * 1.41; // RELU\n",
    "        else if (j!=9) scale = scale * 4.0; // TANH\n",
    "        for (i=0;i<layerSizes[j] * (layerSizes[j-1]+1);i++)\n",
    "            weights[j][i] = scale * ( (float)rand()/RAND_MAX - 0.5 );\n",
    "        for (i=0;i<layerSizes[j];i++) // BIASES\n",
    "            weights[j][(layerSizes[j-1]+1)*(i+1)-1] = 0.0;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "57c6854704c98a1810a05e29fb36224ad4180493"
   },
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87f5cd3963652951a2e3dcfc7ab39fced5da1de8"
   },
   "source": [
    "```\n",
    "int activation = 1; //ReLU\n",
    "// FORWARD PROPAGATION\n",
    "int forwardProp(int x){\n",
    "    int i,j,k,imax;\n",
    "    float sum, esum, max;\n",
    "    // INPUT LAYER - RECEIVES 28X28 IMAGES\n",
    "    for (i=0;i<784;i++) layers[10-numLayers][i] = trainImages[x][i];\n",
    "    // HIDDEN LAYERS - RELU ACTIVATION\n",
    "    for (k=11-numLayers;k<9;k++)\n",
    "    for (i=0;i<layerSizes[k];i++){\n",
    "        sum = 0.0;\n",
    "        for (j=0;j<layerSizes[k-1]+1;j++)\n",
    "            sum += layers[k-1][j]*weights[k][i*(layerSizes[k-1]+1)+j];\n",
    "        if (activation==1) layers[k][i] = ReLU(sum);\n",
    "        else if (activation==2) layers[k][i] = TanH(sum);\n",
    "    }\n",
    "    // OUTPUT LAYER - SOFTMAX ACTIVATION\n",
    "    esum = 0.0;\n",
    "    for (i=0;i<layerSizes[9];i++){\n",
    "        sum = 0.0;\n",
    "        for (j=0;j<layerSizes[8]+1;j++)\n",
    "            sum += layers[8][j]*weights[9][i*(layerSizes[8]+1)+j];\n",
    "        if (sum>30) return -1; //GRADIENT EXPLODED\n",
    "        layers[9][i] = exp(sum);\n",
    "        esum += layers[9][i];\n",
    "    }\n",
    "    // SOFTMAX FUNCTION\n",
    "    max = layers[9][0]; imax=0;\n",
    "    for (i=0;i<layerSizes[9];i++){\n",
    "        if (layers[9][i]>max){\n",
    "            max = layers[9][i];\n",
    "            imax = i;\n",
    "        }\n",
    "        layers[9][i] = layers[9][i] / esum;\n",
    "    }\n",
    "    return imax;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24ed6427d2732cb1802e43ee98bfd3f3134f7ef0"
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea1684d6c00b405f939971dd3375c147a2e49c6d"
   },
   "source": [
    "```\n",
    "float learn = 0.01;\n",
    "float decay = 0.95;\n",
    "// BACKPROPAGATION\n",
    "int backProp(int x, int epoch, float *ent){\n",
    "    int i, j, k, r = 0;\n",
    "    float der=1.0;\n",
    "    // FORWARD PROP FIRST\n",
    "    int p = forwardProp(x);\n",
    "    if (p==-1) return -1; // GRADIENT EXPLODED\n",
    "    // CORRECT PREDICTION?\n",
    "    int y = trainDigits[x];\n",
    "    if (p==y) r=1;\n",
    "    // OUTPUT LAYER - CALCULATE ERRORS\n",
    "    for (i=0;i<layerSizes[9];i++){\n",
    "        errors[9][i] = learn * (0-layers[9][i]) * pow(decay,epoch);\n",
    "        if (i==y) {\n",
    "            errors[9][i] = learn * (1-layers[9][i]) * pow(decay,epoch);\n",
    "            if (layers[9][i]==0) return -1; // GRADIENT VANISHED\n",
    "            *ent = -log(layers[9][i]);\n",
    "        }\n",
    "    }\n",
    "    // HIDDEN LAYERS - CALCULATE ERRORS\n",
    "    for (k=8;k>10-numLayers;k--)\n",
    "    for (i=0;i<layerSizes[k];i++){\n",
    "        errors[k][i] = 0;\n",
    "        if (activation==2) der = (layers[k][i]+1)*(1-layers[k][i]); // TanH DERIVATIVE\n",
    "        if (layers[k][i]>0 || activation==2) // ReLU DERIVATIVE\n",
    "        for (j=0;j<layerSizes[k+1];j++)\n",
    "            errors[k][i] += errors[k+1][j]*weights[k+1][j*(layerSizes[k]+1)+i]*der;\n",
    "    }\n",
    "    // UPDATE WEIGHTS - GRADIENT DESCENT\n",
    "    for (k=11-numLayers;k<10;k++)\n",
    "    for (i=0;i<layerSizes[k];i++)\n",
    "        for (j=0;j<layerSizes[k-1]+1;j++)\n",
    "            weights[k][i*(layerSizes[k-1]+1)+j] += errors[k][i]*layers[k-1][j];\n",
    "    return r;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac1f5004efcdad2a745be6c8ad5f99ea760b3e35"
   },
   "source": [
    "# Training, Validation, Prediction\n",
    "\n",
    "First Kaggle's training set of 42000 images is split into a training subset of 31500 images = 75% x 42,000 and a validation subset of 10500 images = 25% x 42000 images. Each epoch, every training subset image is fed forward into the network and the weights are adjusted by back propagating errors. The learning rate begins at 0.01 and decreases each epoch by 0.95. The batch size is 1 and the order the images are fed into the network is randomized each epoch. By monitoring validation accuracy, we can tune the network's architecture and regularization. Lastly, we classify Kaggle's test set of 28,000 images with the network and submit online. With only dropout (25%), this network achieves 98.25% accuracy. With dropout and data augmentation, this network reaches 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28c6c0f0e1869568eea86b0fe8524cf6e39a9e39"
   },
   "source": [
    "\n",
    "# Regularization\n",
    "To prevent overfitting and to improve generalization, a combination of dropout and data augmentation are employed. Data augmentation is the technique of creating new training images by randomly distorting the ones you have. My program randomly shifts, scales, and rotates the training data. Below is an illustration. The images on the left are the first 24 original MNIST digits. The images on the right have been augmented.\n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/MNISTaugment2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e93960e0a0d8673011bf005651f79ba7d2c835c"
   },
   "source": [
    "# CNN Architecture - 99.5%\n",
    "It's amazing how much more accurate CNNs are over fully connected NNs. By adding convolutions to my C program, I was able to build and train a CNN network. Based on LeNet5's architecture (picture below) I created the following nine layer network 784-10C3-10C3-P2-20C3-20C3-P2-128-10 (not pictured).  \n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/LeNet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93107621371467e52666d4f4a718d430dcc05dca"
   },
   "source": [
    "# Training and Validation  \n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/MNISTconfusion3.png)\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/MNISTiterations2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7f6bc3462629abc3232135df66e75a28e4b4c89"
   },
   "source": [
    "# Errors\n",
    "Looking at the confusion matrix above, you see that the network had the most difficulty distinquishing fours and nines. The six digits below to the left are fours misclassified as nines. And the six on the right are nines misclassified as fours.\n",
    "  \n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/4and9.png)\n",
    "  \n",
    "None-the-less, this CNN achieves a training accuracy of 99.25% and a validation accuracy of 99.5%. Next we classify Kaggle's test images and submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3e8b1493a048f1b02d35aa6e01f1e3102ba1245"
   },
   "source": [
    "# Prediction\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/MNISTresult2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1fecf12c9ddf149b08cff0b30ccb72cde93b75f"
   },
   "source": [
    "# Conclusion\n",
    "It was fun to watch models progressively achieve better accuracy classifying MNIST's handwritten digits. And programming everything in C from scratch taught me a lot. First kNN achieved 97%, next a fully connected net achieved 98.5%. Adding data augmentation increased the fully connected net's accuracy to 99%. And finally I saw a CNN achieve 99.5%. That was impressive! I uploaded my C code to GitHub [here][3]. \n",
    "  \n",
    "UPDATE: Using TensorFlow (and GPUs) I built an ensemble of CNNs and scored an amazing 99.75% accuracy!! Check it out, [here][1].  \n",
    "  \n",
    "UPDATE: I created a webpage where you can draw digits with your mouse and watch this network classify them, [here][2]:\n",
    "\n",
    "[1]:https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist\n",
    "[2]:http://www.ccom.ucsd.edu/~cdeotte/programs/MNIST.html\n",
    "[3]:https://github.com/cdeotte/MNIST-CNN-99.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "61f111782100645a4c1475c36083147a0ba3bd5b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for reading my kernel. I look forward to comments and questions"
     ]
    }
   ],
   "source": [
    "cat(\"Thanks for reading my kernel. I look forward to comments and questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80c7d2d99fb51def9421171ea0322d8881670842"
   },
   "source": [
    "# How does a CNN see MNIST?\n",
    "LeNet5 uses the architecture 784-6C5-P2-16C5-P2-120-84-10. A layer of 6 convolution filters of size 5x5 is the first layer to process an incoming image. These filters only see 5x5 regions in the image and each filter looks for a specific pattern and sends a strong activation when found. Let's call these six filters: red, orange, yellow, green, blue, and purple. Below are examples of patterns that four of these six filters (from one particular trained network) look for.  \n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/filter1.png)\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/filter2.png)\n",
    "  \n",
    "Notice how the yellow filter looks for a diagonal line. The orange filter looks for a left vertical edge. The blue filter looks for a bottom right diagonal edge. And the purple filter looks for a horizontal line.\n",
    "\n",
    "The output from these fiters are fed into a second layer of 16 convolution filters (after passing through a max pooling layer). Each of these 16 filters can only see a 14x14 region in the image. These filters look for certain combinations of the features discovered by the first set of filters. Below are examples of patterns that four of these sixteen filters (from one particular trained network) look for.  \n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/filter3.png)\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/filter4.png)\n",
    "  \n",
    "Notice how the first filter (top left) looks for a curved diagonal line. The second filter (top right) looks for a small loop in the bottom of the range with a line in the top of the range. The third filter (bottom left) looks for a small loop in the bottom of the range with a vertical line up the left side. And the fourth filter (bottom right) looks for a pair of parallel horizontal lines.  \n",
    "  \n",
    "All these filters search for specific things and then they report their findings to the network's final dense layers which classfiy the digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b20f849fd7db560f15e20e8842e196deab79fb9"
   },
   "source": [
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/layers.jpeg)\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/filters.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
